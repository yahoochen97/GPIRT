powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = Ns[k]
n = 4
sigma = 2
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
M = 20
mu = 1
Ns = seq(from = 100, to = 2000, length.out = M)
powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = Ns[k]
n = 4
sigma = 2
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
M = 20
mu = 1
ns = seq(from = 10, to = 200, length.out = M)
powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = 400
n = ns[k]
sigma = 2
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
M = 20
mu = 1
sigmas = seq(from = 0.1, to = 2, length.out = M)
powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = 400
n = 40
sigma = sigmas[k]
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
M = 20
mu = 1
sigmas = seq(from = 1, to = 20, length.out = M)
powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = 400
n = 40
sigma = sigmas[k]
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
M = 20
mu = 1
sigmas = seq(from = 1, to = 10, length.out = M)
powers = rep(0, M)
for(k in 1:M){
set.seed(1)
N = 400
n = 40
sigma = sigmas[k]
null_mu = 0
ps = rep(0, N)
for (i in 1:N){
data = rnorm(n, mean = mu, sd = sigma)
ps[i] = 1-pnorm(mean(data),mean=null_mu,sd=sigma/sqrt(n))
}
powers[k] = sum(ps<=0.05)/N
}
plot(1:M, powers)
library(estimatr)
?lm_robust
?gsynth
library(gsynth)
?gsynth
library(bpCausal)
?bpCausal
library(gsynth)
?gsynth
library(bpCausal)
?bpCausal
knitr::opts_chunk$set(echo = TRUE)
# plot the samples
plot(rep(1,num_samples),samples)
# draw the samples
mu = 0
sd_dev = 1
num_samples = 100
samples = rnorm(n = num_samples, mean = mu, sd = sd_dev)
# plot the samples
plot(rep(1,num_samples),samples)
library(MASS)
# draw the samples
mu = c(0,0)
covariance = matrix(c(1,0,0,1), nrow=2)
num_samples = 1000
samples = mvrnorm(n = num_samples, mu = mu, Sigma = covariance)
# draw the samples
mu = c(0,0)
covariance = matrix(c(1,0.9,0.9,1), nrow=2)
num_samples = 1000
samples = mvrnorm(n = num_samples, mu = mu, Sigma = covariance)
library(KRLS)
dimension = 10
mu = rep(0,dimension)
covariance = gausskernel(1:dimension, sigma=10)
num_samples = 5
samples = mvrnorm(n = num_samples, mu = mu, Sigma = covariance)
plot(1:dimension, samples[1,], 'l', ylim=c(-2,2))
for (i in 2:num_samples) {
lines(1:dimension, samples[i,])
}
library(KRLS)
dimension = 100
mu = rep(0,dimension)
covariance = gausskernel(1:dimension, sigma=50)
num_samples = 2
samples = mvrnorm(n = num_samples, mu = mu, Sigma = covariance)
plot(1:dimension, samples[1,], 'l', ylim=c(-2,2))
for (i in 2:num_samples) {
lines(1:dimension, samples[i,])
}
library(KRLS)
dimension = 100
mu = rep(0,dimension)
covariance = gausskernel(1:dimension, sigma=50)
num_samples = 2
samples = mvrnorm(n = num_samples, mu = mu, Sigma = covariance)
plot(1:dimension, samples[1,], 'l', ylim=c(-2,2))
for (i in 2:num_samples) {
lines(1:dimension, samples[i,])
}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(fastDummies)
# Step 1) Set up your data -- take care of your missing values
newData <- fastDummies::dummy_cols(ctsib)
library(faraway)
data(ctsib, package="faraway")
nrow(ctsib) # Number of observations
table(ctsib$CTSIB) # Our dependent variable
length(table(ctsib$Subject)) # Number of unique "subjects" in the experiment.
library(estimatr)
fit1 = lm_robust(CTSIB ~ Surface + Vision, fixed_effects = ~Subject, clusters = Subject, data = ctsib)
summary(fit1)
fit2 = lm(CTSIB ~ Surface + Vision + factor(Subject), data = ctsib)
summary(fit2)
library(lme4)
library(fastDummies)
# Step 1) Set up your data -- take care of your missing values
newData <- fastDummies::dummy_cols(ctsib)
X<-newData[,c( "Surface_foam", "Vision_dome", "Vision_open", "Sex_male", "Age", "Height", "Weight")]
Subject<-newData$Subject
## Step 2) Write your model
## Step 3) Set up data so it works with the model
simple_data<-list(
N=nrow(X),
J=40,
K=ncol(X),
y=newData$CTSIB,
X=X,
Subject=Subject
)
library(rstan)
## Step 4) Run the model
stanFit1<-stan(
file="MLM_model.stan",
data=simple_data,
chains=3,
warmup=2000,
iter=4000,
cores=1
)
summary(stanFit1)
summary(stanFit1)$summary
knitr::opts_chunk$set(echo = TRUE)
data <- iris
knitr::opts_chunk$set(echo = TRUE)
summary(iris)
summary(iris)
iris.mis <- prodNA(iris, noNA = 0.1)
library(missForest)
iris.mis <- prodNA(iris, noNA = 0.1)
summary(iris.mis)
library(missForest)
iris.mis <- prodNA(iris, noNA = 0.1)
iris.mis <- subset(iris.mis, select = -c(Species))
summary(iris.mis)
install.packages("mice")
library(mice)
library(mice)
imputed_Data <- mice(iris.mis, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
library(mice)
imputed_Data <- mice(iris.mis, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
imputed_Data$imp$Sepal.Width
library(mice)
imputed_Data <- mice(iris.mis, m=5, maxit = 50, method = 'pmm', seed = 500)
imputed_Data$imp$Sepal.Width
completeData <- complete(imputed_Data,2)
fit <- with(data = iris.mis, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width))
combine <- pool(fit)
completeData <- complete(imputed_Data,2)
fit <- lm(Sepal.Width ~ Sepal.Length + Petal.Width, data=completeData)
summary(fit)
fit <- lm(Sepal.Width ~ Sepal.Length + Petal.Width, data=iris)
summary(fit)
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
data(ctsib, package="faraway")
nrow(ctsib) # Number of observations
table(ctsib$CTSIB) # Our dependent variable
length(table(ctsib$Subject)) # Number of unique "subjects" in the experiment.
head(ctsib)
library(glmnet)
lasso = cv.glmnet(ctsib[,-c("CTSIB")],ctsib$CTSIB,alpha=1)
head(ctsib)
library(glmnet)
lasso = cv.glmnet(ctsib[,-c(1,8)],ctsib$CTSIB,alpha=1)
?cv.glmnet
library(glmnet)
x = ctsib[,-c(1,8)]
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = data.frame(ctsib[,-c(1,8)])
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
y = ctsib$CTSIB
lasso = cv.glmnet(newData,y,alpha=1)
library(lme4)
library(fastDummies)
# Step 1) Set up your data -- take care of your missing values
newData <- fastDummies::dummy_cols(ctsib)
X<-newData[,c( "Surface_foam", "Vision_dome", "Vision_open", "Sex_male", "Age", "Height", "Weight")]
Subject<-newData$Subject
## Step 2) Write your model
## Step 3) Set up data so it works with the model
simple_data<-list(
N=nrow(X),
J=40,
K=ncol(X),
y=newData$CTSIB,
X=X,
Subject=Subject
)
library(rstan)
## Step 4) Run the model
stanFit1<-stan(
file="MLM_model.stan",
data=simple_data,
chains=3,
warmup=2000,
iter=4000,
cores=1
)
library(glmnet)
y = ctsib$CTSIB
lasso = cv.glmnet(newData,y,alpha=1)
library(glmnet)
y = ctsib$CTSIB
lasso = cv.glmnet(as.numeric(newData),y,alpha=1)
library(glmnet)
y = as.numeric(ctsib$CTSIB)
lasso = cv.glmnet(as.numeric(newData),y,alpha=1)
class(y)
class(newData)
library(glmnet)
x = newData[,-c(8)]
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = as.matrix(newData[,-c(8)])
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = as.numeric(as.matrix(newData[,-c(8)]))
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
size(x)
x
library(glmnet)
x = as.numeric(data.matrix(newData[,-c(8)]))
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
newData[,-c(8)]
library(glmnet)
x = as.numeric(data.matrix(newData[,-c(1,8)]))
y = ctsib$CTSIB
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = as.numeric(data.matrix(newData[,-c(1,8)]))
y = data.matrix(ctsib$CTSIB)
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = as.numeric(data.matrix(newData[,-c(1,2,6,7,8)]))
y = data.matrix(ctsib$CTSIB)
lasso = cv.glmnet(x,y,alpha=1)
library(glmnet)
x = data.matrix(newData[,-c(1,2,6,7,8)])
y = data.matrix(ctsib$CTSIB)
lasso = cv.glmnet(x,y,alpha=1)
summary(lasso)
mean(ctsib$Height)
mean(ctsib$Weight)
library(KRLS)
fit = krls(x, y)
head(x)
predict.krls(fit, c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1))
xstar = c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1)
predict.krls(fit, )
xstar
xstar = c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1)
predict.krls(fit, xstar)
xstar = c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1)
predict.krls(fit, xstar)
xstar = transpose(c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1))
xstar = matrix(c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1))
predict.krls(fit, xstar)
xstar = matrix(c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1), ncol=1)
predict.krls(fit, xstar)
xstar = matrix(c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1), nrow=1)
predict.krls(fit, xstar)
xstar = matrix(c(40, 180, 75, 0, 1, 0, 1, 0, 0, 1), nrow=1)
ystar = predict.krls(fit, xstar)
xstar = matrix(c(40, 180, 75, 0, 1, 0, 1, 1, 0, 0), nrow=1)
ystar = predict.krls(fit, xstar)
ystar$fit
xstar = matrix(c(40, 180, 75, 0, 1, 1, 0, 1, 0, 0), nrow=1)
ystar = predict.krls(fit, xstar)
ystar$fit
library(lme4)
library(fastDummies)
# Step 1) Set up your data -- take care of your missing values
newData <- fastDummies::dummy_cols(ctsib)
X<-newData[,c( "Surface_foam", "Vision_dome", "Vision_open", "Sex_male", "Age", "Height", "Weight")]
Subject<-newData$Subject
## Step 2) Write your model
## Step 3) Set up data so it works with the model
simple_data<-list(
N=nrow(X),
J=40,
K=ncol(X),
y=newData$CTSIB,
X=X,
Subject=Subject
)
library(rstan)
## Step 4) Run the model
stanFit1<-stan(
file="MLM_model.stan",
data=simple_data,
chains=2,
warmup=500,
iter=1000,
cores=1
)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
data = read_dta("hierarchypaperdata.dta")
data = data[data$year==2012, c("polempowerment", "polity","cleanelec", "lpop","neighborpolempowerment")]
cleaned_data <- na.omit(data)
cleaned_data = cleaned_data[cleaned_data$polity<=100, ]
cleaned_data = cleaned_data[cleaned_data$polity>=0, ]
?mice
library(MICE)
library(mice)
?mice
library(mice)
imputed_Data <- mice(data, m=5, maxit = 50, method = 'pmm', seed = 500, printFlag = FALSE)
install.packages("gpirt", llib = "/Users/yahoo/Documents/GitHub/gpirtr")
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
gpirt_path = "~/Documents/Github/gpirt"
setwd(gpirt_path)
setwd("../OrdGPIRT")
library(gpirt)
library(dplyr)
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/bgrm_logit.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/bgrm_logit.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/bgrm_logit.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/bgrm_logit.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
HYP
print(cor(theta,pred_theta))
print(mean(train_lls))
print(mean(train_acc))
print(mean(pred_lls))
print(mean(pred_acc))
source('~/Documents/GitHub/OrdGPIRT/generate_data.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
results = grm(data = data_train, na.action = NULL)
betas = results$coefficients
pred_theta = factor.scores(results, resp.patterns = data_train)
pred_theta = pred_theta$score.dat[,m+3]
pred_lls = c()
pred_acc = c()
train_lls = c()
train_acc = c()
xs = seq(-5,5,0.01)
idx = (as.integer(min(theta)*100+500)):(as.integer(max(theta)*100+500))
grm_iccs = matrix(0, nrow=length(idx), ncol=m)
true_iccs = matrix(0, nrow=nrow(grm_iccs), ncol=m)
cor_icc = rep(0, m)
rmse_icc = rep(0, m)
for (i in 1:nrow(data)) {
for (j in 1:ncol(data)) {
if(!is.na(data[[i,j]])){
ps = rep(0, C+1)
ps[C+1] = 1
beta_coef = betas[[paste('Item ', j, sep='')]]
for (c in 1:(C-1)){
lp = beta_coef[[c]] - pred_theta[i]*beta_coef[[C]]
ps[c+1] = 1 / (1+ exp(-lp))
}
ll = rep(0, C)
for (c in 1:C) {
ll[c] = log(ps[c+1] - ps[c])
}
y_pred = which.max(ll)
if(train_idx[i,j]==0){
pred_acc = c(pred_acc, y_pred==(data[[i,j]]))
pred_lls = c(pred_lls, ll[data[[i,j]]])
}else{
train_acc = c(train_acc, y_pred==(data[[i,j]]))
train_lls = c(train_lls, ll[data[[i,j]]])
}
}
}
}
for (j in 1:m) {
source("true_irf.R")
probs = getprobs_gpirt(sign(cor(theta,pred_theta))*xs[idx], irfs, matrix(thresholds[j,],nrow=1))
tmp = probs %>%
group_by(xs) %>%
summarize(icc=sum(order*p))
true_iccs[,j] = tmp$icc
tmp = c()
for (i in 1:length(xs[idx])) {
ps = rep(0, C+1)
ps[C+1] = 1
beta_coef = betas[[paste('Item ', j, sep='')]]
for (c in 1:(C-1)){
lp = beta_coef[[c]] - xs[idx][i]*beta_coef[[C]]
ps[c+1] = 1 / (1+ exp(-lp))
}
p = rep(0, C)
for (c in 1:C) {
p[c] = ps[c+1] - ps[c]
}
tmp = c(tmp, sum(p*(1:C)))
}
grm_iccs[,j] = tmp
cor_icc[j] = cor(grm_iccs[,j], true_iccs[,j])
rmse_icc[j] = sqrt(mean((grm_iccs[,j]-true_iccs[,j])^2))
}
print(cor(theta,pred_theta))
print(mean(train_lls))
print(mean(train_acc))
print(mean(pred_lls))
print(mean(pred_acc))
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
source('~/Documents/GitHub/OrdGPIRT/2PL.R', echo=TRUE)
results = grm(data = data_train, na.action = NULL)
results
results = grm(data = data_train, na.action = NULL)
results
results$coefficients
print( results$coefficients)
print(results$coefficients)
